{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "iha1krMekyeI"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TeFFstuk1tU",
        "outputId": "6ee2b05b-c8ce-45c6-affa-dc3c86e1ca97"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fusce commodo mauris id justo condimentum dignissim. Nullam placerat semper dapibus. Pellentesque ac risus nulla. Phasellus ut dapibus nunc, id aliquam dolor.\""
      ],
      "metadata": {
        "id": "Q8P3tg83lBEr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "id": "xEh4k6sllC7k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f66bfbd-3cc7-44df-d8ee-690af710348e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Lorem', 'ipsum', 'dolor', 'sit', 'amet', ',', 'consectetur', 'adipiscing', 'elit', '.', 'Fusce', 'commodo', 'mauris', 'id', 'justo', 'condimentum', 'dignissim', '.', 'Nullam', 'placerat', 'semper', 'dapibus', '.', 'Pellentesque', 'ac', 'risus', 'nulla', '.', 'Phasellus', 'ut', 'dapibus', 'nunc', ',', 'id', 'aliquam', 'dolor', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiX8kJAklLoQ",
        "outputId": "a7c0d364-43e6-4d28-a4bb-3f85353aecc1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Lorem ipsum dolor sit amet, consectetur adipiscing elit.', 'Fusce commodo mauris id justo condimentum dignissim.', 'Nullam placerat semper dapibus.', 'Pellentesque ac risus nulla.', 'Phasellus ut dapibus nunc, id aliquam dolor.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "to_tag = word_tokenize(text)"
      ],
      "metadata": {
        "id": "orePE0C5lPWx"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pos_tag(to_tag))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dZrNAq_lRTK",
        "outputId": "4aabc0b4-00c8-4721-aa20-abf1cebfe188"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Lorem', 'NNP'), ('ipsum', 'NN'), ('dolor', 'NN'), ('sit', 'NN'), ('amet', 'NN'), (',', ','), ('consectetur', 'NN'), ('adipiscing', 'VBG'), ('elit', 'NN'), ('.', '.'), ('Fusce', 'NNP'), ('commodo', 'JJ'), ('mauris', 'NN'), ('id', 'NN'), ('justo', 'NN'), ('condimentum', 'NN'), ('dignissim', 'NN'), ('.', '.'), ('Nullam', 'NNP'), ('placerat', 'VBZ'), ('semper', 'JJR'), ('dapibus', 'NN'), ('.', '.'), ('Pellentesque', 'NNP'), ('ac', 'JJ'), ('risus', 'NN'), ('nulla', 'NN'), ('.', '.'), ('Phasellus', 'CC'), ('ut', 'JJ'), ('dapibus', 'NN'), ('nunc', 'NN'), (',', ','), ('id', 'JJ'), ('aliquam', 'NN'), ('dolor', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoYi5q21lTeU",
        "outputId": "b07bad88-4cf4-4371-fc20-1b445c91780c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'you', 'doing', 'with', \"aren't\", 'what', 'ma', 'but', 'itself', 'that', 'here', 'a', 'hadn', 'do', \"shan't\", 'my', 'them', 'couldn', 'isn', \"wouldn't\", 'myself', 'or', 've', 'hasn', 'its', \"haven't\", 'ourselves', \"shouldn't\", \"didn't\", 'haven', 'only', 'up', 'it', 'few', 'yourselves', 'between', 'mightn', \"hasn't\", 'had', 'all', 'theirs', 'through', \"wasn't\", 'no', 'an', 'were', 'their', 'having', 'because', 'his', 'am', \"mustn't\", 'other', 'how', 'himself', 'ain', 'doesn', 'where', 'ours', 'which', 'who', 'out', 'herself', 'while', 'i', 'will', 'did', 'weren', \"it's\", 'such', 'these', 'under', 'both', 'd', 'this', 'when', 'now', \"hadn't\", 'in', 'each', 's', 'o', 'just', 'more', 'wasn', 're', 'yours', 'being', 'very', 'mustn', \"you'd\", 'they', 'down', 'once', \"mightn't\", \"needn't\", 'whom', \"you'll\", 'to', 't', 'shan', 'he', 'shouldn', 'are', 'some', \"that'll\", 'has', 'should', 'be', 'didn', \"won't\", 'during', 'we', 'why', \"she's\", 'nor', 'yourself', 'at', 'll', \"doesn't\", 'themselves', 'into', 'not', \"you've\", 'as', 'below', 'was', 'by', \"weren't\", 'her', 'won', 'those', 'and', 'needn', 'm', 'own', 'is', \"isn't\", 'over', 'can', 'the', 'does', 'same', \"don't\", 'above', 'y', 'aren', 'of', 'than', 'there', \"should've\", 'for', \"you're\", 'don', 'me', 'she', 'have', 'after', 'too', 'until', 'hers', 'most', 'wouldn', 'your', 'been', 'from', 'then', 'him', 'our', 'again', 'against', 'on', 'further', 'so', 'any', 'before', 'about', \"couldn't\", 'if', 'off'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "to_clean = word_tokenize(text)"
      ],
      "metadata": {
        "id": "zr54x48IlVN9"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_stopwords_text = []\n",
        "for token in to_clean:\n",
        "    if(token not in stop_words):\n",
        "        no_stopwords_text.append(token)\n",
        "\n",
        "print(no_stopwords_text)"
      ],
      "metadata": {
        "id": "NKk-KvCllXTF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eba0957e-cc8e-4a84-dc01-392c0d810b3c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Lorem', 'ipsum', 'dolor', 'sit', 'amet', ',', 'consectetur', 'adipiscing', 'elit', '.', 'Fusce', 'commodo', 'mauris', 'id', 'justo', 'condimentum', 'dignissim', '.', 'Nullam', 'placerat', 'semper', 'dapibus', '.', 'Pellentesque', 'ac', 'risus', 'nulla', '.', 'Phasellus', 'ut', 'dapibus', 'nunc', ',', 'id', 'aliquam', 'dolor', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "mIInBbSDlZyp"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_words = []\n",
        "for token in no_stopwords_text:\n",
        "    stemmed_word = stemmer.stem(token)\n",
        "    stemmed_words.append(stemmed_word)"
      ],
      "metadata": {
        "id": "qiCM9oT_lcJ_"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stemmed_words)"
      ],
      "metadata": {
        "id": "hJgLHrBJld5T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47183c80-f7cd-40a4-9f00-86aa6279a3dc"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['lorem', 'ipsum', 'dolor', 'sit', 'amet', ',', 'consectetur', 'adipisc', 'elit', '.', 'fusc', 'commodo', 'mauri', 'id', 'justo', 'condimentum', 'dignissim', '.', 'nullam', 'placerat', 'semper', 'dapibu', '.', 'pellentesqu', 'ac', 'risu', 'nulla', '.', 'phasellu', 'ut', 'dapibu', 'nunc', ',', 'id', 'aliquam', 'dolor', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "Zq8k9QjXlgJt"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_words = []\n",
        "for token in no_stopwords_text:\n",
        "    lemmatized = lemmatizer.lemmatize(token)  # Assuming you want to lemmatize verbs (you can change the 'pos' argument as needed)\n",
        "    lemmatized_words.append(lemmatized)"
      ],
      "metadata": {
        "id": "3ibZCtvntzG-"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "av3YQV6gt1aR",
        "outputId": "5f1e6887-74e1-46f1-e18c-95d0a49913f6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Lorem', 'ipsum', 'dolor', 'sit', 'amet', ',', 'consectetur', 'adipiscing', 'elit', '.', 'Fusce', 'commodo', 'mauris', 'id', 'justo', 'condimentum', 'dignissim', '.', 'Nullam', 'placerat', 'semper', 'dapibus', '.', 'Pellentesque', 'ac', 'risus', 'nulla', '.', 'Phasellus', 'ut', 'dapibus', 'nunc', ',', 'id', 'aliquam', 'dolor', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()"
      ],
      "metadata": {
        "id": "0Fy38axct3He"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"I love to eat pizza\",\n",
        "    \"Pizza is my favorite food\",\n",
        "    \"I enjoy eating pizza with friends\",\n",
        "    \"I like to have pizza for dinner\",\n",
        "    \"Pizza toppings include cheese, pepperoni, and mushrooms\"\n",
        "]"
      ],
      "metadata": {
        "id": "qOLqQPcLt4wz"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()"
      ],
      "metadata": {
        "id": "t4R3oPeAt61Z"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "id": "b2koEU9ft9Dd"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tfidf_matrix.toarray())\n",
        "\n",
        "print(feature_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UBaM0qFt-bz",
        "outputId": "126b0e5c-ff16-4cc9-c947-a1fce42bbffc"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         0.58946308 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.58946308 0.         0.         0.\n",
            "  0.28088232 0.4755751  0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.48638585 0.48638585 0.         0.         0.         0.\n",
            "  0.48638585 0.         0.         0.         0.48638585 0.\n",
            "  0.23176546 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.48638585 0.48638585\n",
            "  0.         0.         0.         0.48638585 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.23176546 0.         0.         0.48638585]\n",
            " [0.         0.         0.45277275 0.         0.         0.\n",
            "  0.         0.         0.45277275 0.         0.45277275 0.\n",
            "  0.         0.45277275 0.         0.         0.         0.\n",
            "  0.21574864 0.36529421 0.         0.        ]\n",
            " [0.40073619 0.40073619 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.40073619\n",
            "  0.         0.         0.         0.40073619 0.         0.40073619\n",
            "  0.19095294 0.         0.40073619 0.        ]]\n",
            "['and' 'cheese' 'dinner' 'eat' 'eating' 'enjoy' 'favorite' 'food' 'for'\n",
            " 'friends' 'have' 'include' 'is' 'like' 'love' 'mushrooms' 'my'\n",
            " 'pepperoni' 'pizza' 'to' 'toppings' 'with']\n"
          ]
        }
      ]
    }
  ]
}