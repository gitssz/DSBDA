{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c32272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b011d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2728b4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating   eat\n",
      "eats   eat\n",
      "eat   eat\n",
      "ate   ate\n",
      "adjustable   adjust\n",
      "rafting   raft\n",
      "ability   abil\n",
      "meeting   meet\n"
     ]
    }
   ],
   "source": [
    "words=['eating','eats','eat','ate','adjustable','rafting','ability','meeting']\n",
    "\n",
    "for word in words:\n",
    "    print(word,\" \", stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d1e2a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating eat\n",
      "eats eat\n",
      "eat eat\n",
      "ate eat\n",
      "adjustable adjustable\n",
      "rafting raft\n",
      "ability ability\n",
      "meeting meeting\n",
      "better well\n"
     ]
    }
   ],
   "source": [
    "#lemmatization\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc=nlp(\"eating eats eat ate adjustable rafting ability meeting better\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token,token.lemma_)\n",
    "#           \"| \",token.lemma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7151f801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mando Mando\n",
      "talked talk\n",
      "for for\n",
      "3 3\n",
      "hours hour\n",
      "alhough alhough\n",
      "taking taking\n",
      "is be\n",
      "nt not\n",
      "his his\n",
      "thig thig\n"
     ]
    }
   ],
   "source": [
    "doc=nlp('Mando talked for 3 hours alhough taking isn't his thing')\n",
    "for token in doc:\n",
    "    print(token, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b043f47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ar= nlp.get_pipe('attribute_ruler')\n",
    "# ar.add([[{\"TEXT\":\"Bro\"}],[{\"TEXT\",\"Bruh\"}]],{'LEMMA':\"Brother\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6092fff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stemmers', 'might', 'not', 'always', 'result', 'in', 'semantically', 'meaningful', 'base', 'words', '.', 'But', 'stemmers', 'are', 'faster', 'and', 'computationally', 'less', 'expensive', 'than', 'lemmatizers', '.']\n",
      "\n",
      "['Stemmers might not always result in semantically meaningful base words.', 'But stemmers are faster and computationally less expensive than lemmatizers.']\n"
     ]
    }
   ],
   "source": [
    "#tokenization\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "sentence=\"Stemmers might not always result in semantically meaningful base words. But stemmers are faster and computationally less expensive than lemmatizers. \"\n",
    "print(word_tokenize(sentence))\n",
    "print()\n",
    "print(sent_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c784b200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmers  |  NOUN  |  noun  |  NNS noun, plural\n",
      "might  |  AUX  |  auxiliary  |  MD verb, modal auxiliary\n",
      "not  |  PART  |  particle  |  RB adverb\n",
      "always  |  ADV  |  adverb  |  RB adverb\n",
      "result  |  VERB  |  verb  |  VB verb, base form\n",
      "in  |  ADP  |  adposition  |  IN conjunction, subordinating or preposition\n",
      "semantically  |  ADV  |  adverb  |  RB adverb\n",
      "meaningful  |  ADJ  |  adjective  |  JJ adjective (English), other noun-modifier (Chinese)\n",
      "base  |  ADJ  |  adjective  |  JJ adjective (English), other noun-modifier (Chinese)\n",
      "words  |  NOUN  |  noun  |  NNS noun, plural\n",
      ".  |  PUNCT  |  punctuation  |  . punctuation mark, sentence closer\n",
      "But  |  CCONJ  |  coordinating conjunction  |  CC conjunction, coordinating\n",
      "stemmers  |  NOUN  |  noun  |  NNS noun, plural\n",
      "are  |  AUX  |  auxiliary  |  VBP verb, non-3rd person singular present\n",
      "faster  |  ADJ  |  adjective  |  JJR adjective, comparative\n",
      "and  |  CCONJ  |  coordinating conjunction  |  CC conjunction, coordinating\n",
      "computationally  |  ADV  |  adverb  |  RB adverb\n",
      "less  |  ADV  |  adverb  |  RBR adverb, comparative\n",
      "expensive  |  ADJ  |  adjective  |  JJ adjective (English), other noun-modifier (Chinese)\n",
      "than  |  ADP  |  adposition  |  IN conjunction, subordinating or preposition\n",
      "lemmatizers  |  NOUN  |  noun  |  NNS noun, plural\n",
      ".  |  PUNCT  |  punctuation  |  . punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "#pos tagging\n",
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc=nlp(\"Stemmers might not always result in semantically meaningful base words. But stemmers are faster and computationally less expensive than lemmatizers.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \" | \",token.pos_,\" | \",spacy.explain(token.pos_),\" | \",token.tag_\n",
    "         ,spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a1b4a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quits  |  VBZ  |  verb, 3rd person singular present\n"
     ]
    }
   ],
   "source": [
    "sent= nlp(\"He quits the job\")\n",
    "sent[1]\n",
    "print(sent[1],\" | \",sent[1].tag_,\" | \",spacy.explain(sent[1].tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4b452264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gitanjali PROPN   proper noun\n",
      "bought VERB   verb\n",
      "fruits NOUN   noun\n",
      ". PUNCT   punctuation\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "text=nlp(\"Gitanjali bought fruits.\")\n",
    "for token in text:\n",
    "    print(token,token.pos_,\" \",spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0c307c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7358e887",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent=\"This place is so nice. The scenery here is very beautiful , we must come back here again.\"\n",
    "word_token=word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2a5da50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['place', 'nice', '.', 'scenery', 'beautiful', ',', 'must', 'come', 'back', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filtered_sentence=[]\n",
    "for w in word_token:\n",
    "    if w.lower() not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ecfd551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t0.5386476208856763\n",
      "  (0, 6)\t0.281088674033753\n",
      "  (0, 3)\t0.281088674033753\n",
      "  (0, 1)\t0.6876235979836938\n",
      "  (0, 8)\t0.281088674033753\n",
      "  (1, 2)\t0.5802858236844359\n",
      "  (1, 6)\t0.38408524091481483\n",
      "  (1, 3)\t0.38408524091481483\n",
      "  (1, 1)\t0.46979138557992045\n",
      "  (1, 8)\t0.38408524091481483\n",
      "  (2, 2)\t0.5802858236844359\n",
      "  (2, 6)\t0.38408524091481483\n",
      "  (2, 3)\t0.38408524091481483\n",
      "  (2, 1)\t0.46979138557992045\n",
      "  (2, 8)\t0.38408524091481483\n",
      "  (3, 4)\t0.511848512707169\n",
      "  (3, 7)\t0.511848512707169\n",
      "  (3, 0)\t0.511848512707169\n",
      "  (3, 6)\t0.267103787642168\n",
      "  (3, 3)\t0.267103787642168\n",
      "  (3, 8)\t0.267103787642168\n",
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "# tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus={\n",
    "    'This is the first document.',\n",
    "             'This document is the second document.',\n",
    "             'And this is the third one.',\n",
    "             'Is this the first document?'\n",
    "}\n",
    "\n",
    "vector=TfidfVectorizer()\n",
    "\n",
    "t=vector.fit_transform(corpus)\n",
    "print(t)\n",
    "\n",
    "feature_names=vector.get_feature_names_out()\n",
    "print(feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841845e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
